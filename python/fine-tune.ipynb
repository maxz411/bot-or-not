{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: v5 Fine Tuning\n",
        "\n",
        "This notebook runs the full v5 OpenAI supervised fine-tuning workflow for bot-or-not:\n",
        "\n",
        "1. Sync Python dependencies\n",
        "2. Prepare strict pair-aware data splits (`30+32` vs `31+33`)\n",
        "3. Run pair-holdout CV fine-tuning\n",
        "4. Train final model on all datasets\n",
        "5. Evaluate and optionally emit a challenge-compatible run file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bdbdd068",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT: /Users/max/code/bot-or-not\n",
            "ARTIFACTS:    /Users/max/code/bot-or-not/python/artifacts/fine_tuning\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "from fine_tuning import prepare_data, run_cv, train_final, eval_model\n",
        "from fine_tuning.constants import (\n",
        "    ARTIFACTS_ROOT,\n",
        "    PREPARED_DIR,\n",
        "    PROJECT_ROOT,\n",
        "    RUNS_DIR,\n",
        ")\n",
        "\n",
        "print(f'PROJECT_ROOT: {PROJECT_ROOT}')\n",
        "print(f'ARTIFACTS:    {ARTIFACTS_ROOT}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74ce0b8f",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set knobs here before running live jobs.\n",
        "\n",
        "- Keep `RUN_LIVE_JOBS = False` for dry-run only\n",
        "- Set `RUN_LIVE_JOBS = True` to create OpenAI fine-tuning jobs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7bd353f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY present: True\n"
          ]
        }
      ],
      "source": [
        "DATASETS = (30, 31, 32, 33)\n",
        "BASE_MODEL = 'gpt-4.1-mini-2025-04-14'\n",
        "# BASE_MODEL = 'gpt-4.1-2025-04-14' # wayyyyy too expensive\n",
        "VAL_FRACTION = 0.10\n",
        "SEED = 20260214\n",
        "\n",
        "# Fine-tuning hyperparameters\n",
        "# Use None for provider default (auto) where supported.\n",
        "FT_EPOCHS = 'auto'  # e.g. 'auto' or 3\n",
        "FT_BATCH_SIZE = None  # e.g. None, 'auto', or 8\n",
        "FT_LEARNING_RATE_MULTIPLIER = None  # e.g. None, 'auto', or 1.0\n",
        "\n",
        "# Live execution controls\n",
        "RUN_LIVE_JOBS = True\n",
        "NO_WAIT = False  # If True, submit jobs and return immediately\n",
        "POLL_SECONDS = 30\n",
        "MAX_WAIT_MINUTES = 0  # 0 means no timeout\n",
        "\n",
        "# Evaluation controls\n",
        "MAX_EVAL_SAMPLES = 0  # 0 means evaluate all users\n",
        "WRITE_RUN_FILE = True\n",
        "\n",
        "openai_key_present = bool(os.environ.get('OPENAI_API_KEY'))\n",
        "print('OPENAI_API_KEY present:', openai_key_present)\n",
        "if not openai_key_present:\n",
        "    print('WARNING: OPENAI_API_KEY is not set. Live job submission/evaluation will fail until it is set.')\n",
        "\n",
        "print('Hyperparameters:')\n",
        "print('  epochs:', FT_EPOCHS)\n",
        "print('  batch_size:', FT_BATCH_SIZE)\n",
        "print('  learning_rate_multiplier:', FT_LEARNING_RATE_MULTIPLIER)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "65894acd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_shell(cmd: str, cwd: Path = PROJECT_ROOT, check: bool = True) -> subprocess.CompletedProcess[str]:\n",
        "    \"\"\"Helper for non-Python commands (e.g. bun).\"\"\"\n",
        "    print(f\"\\n$ {cmd}\")\n",
        "    completed = subprocess.run(cmd, cwd=str(cwd), shell=True, text=True, capture_output=True)\n",
        "    if completed.stdout:\n",
        "        print(completed.stdout)\n",
        "    if completed.returncode != 0:\n",
        "        if completed.stderr:\n",
        "            print(completed.stderr, file=sys.stderr)\n",
        "        if check:\n",
        "            raise RuntimeError(f'Command failed ({completed.returncode}): {cmd}')\n",
        "    return completed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bddd949",
      "metadata": {},
      "source": [
        "## 1) Sync dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be2779d",
      "metadata": {},
      "source": [
        "## 2) Prepare data (full-post inputs, strict pair holdout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0a63e43d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared data written to: /Users/max/code/bot-or-not/python/artifacts/fine_tuning/prepared\n",
            "Data integrity: total=889 BOT=184 HUMAN=705\n",
            "No-truncation mismatches: 0\n",
            "fold_a: train=491 val=55 test=343 | overlaps train/val=0 train/test=0 val/test=0\n",
            "fold_b: train=308 val=35 test=546 | overlaps train/val=0 train/test=0 val/test=0\n",
            "final: train=799 val=90 overlap=0\n",
            "{\n",
            "  \"total_examples\": 889,\n",
            "  \"labels\": {\n",
            "    \"BOT\": 184,\n",
            "    \"HUMAN\": 705\n",
            "  },\n",
            "  \"languages\": {\n",
            "    \"en\": 546,\n",
            "    \"fr\": 343\n",
            "  },\n",
            "  \"datasets\": {\n",
            "    \"30\": 275,\n",
            "    \"33\": 172,\n",
            "    \"31\": 171,\n",
            "    \"32\": 271\n",
            "  },\n",
            "  \"no_truncation_mismatches\": 0\n",
            "}\n",
            "Data integrity and split integrity checks passed.\n"
          ]
        }
      ],
      "source": [
        "summary = prepare_data(\n",
        "    datasets=DATASETS,\n",
        "    prepared_dir=PREPARED_DIR,\n",
        "    val_fraction=VAL_FRACTION,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "print(json.dumps(summary['data_integrity'], indent=2))\n",
        "\n",
        "assert summary['data_integrity']['total_examples'] == 889\n",
        "assert summary['data_integrity']['labels']['BOT'] == 184\n",
        "assert summary['data_integrity']['labels']['HUMAN'] == 705\n",
        "assert summary['data_integrity']['no_truncation_mismatches'] == 0\n",
        "\n",
        "for fold in summary['pair_folds']:\n",
        "    assert fold['train_test_overlap'] == 0\n",
        "    assert fold['val_test_overlap'] == 0\n",
        "    assert fold['train_val_overlap'] == 0\n",
        "\n",
        "print('Data integrity and split integrity checks passed.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7399f38d",
      "metadata": {},
      "source": [
        "## 3) Run strict pair-holdout CV fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86aa8bf7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitting fold_a fine-tuning job...\n"
          ]
        }
      ],
      "source": [
        "cv_summary = run_cv(\n",
        "    prepared_dir=PREPARED_DIR,\n",
        "    base_model=BASE_MODEL,\n",
        "    n_epochs=FT_EPOCHS,\n",
        "    batch_size=FT_BATCH_SIZE,\n",
        "    learning_rate_multiplier=FT_LEARNING_RATE_MULTIPLIER,\n",
        "    poll_seconds=POLL_SECONDS,\n",
        "    max_wait_minutes=MAX_WAIT_MINUTES,\n",
        "    max_samples=MAX_EVAL_SAMPLES,\n",
        "    dry_run=not RUN_LIVE_JOBS,\n",
        "    no_wait=NO_WAIT,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab197e9f",
      "metadata": {},
      "source": [
        "## 4) Train final model on all datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4947e11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "$ python3 -m fine_tuning train-final --prepared-dir /Users/max/code/bot-or-not/python/artifacts/fine_tuning/prepared --base-model gpt-4.1-mini-2025-04-14 --poll-seconds 30 --max-wait-minutes 0 --dry-run\n",
            "Would submit final train job using: /Users/max/code/bot-or-not/python/artifacts/fine_tuning/prepared/final/train.jsonl + /Users/max/code/bot-or-not/python/artifacts/fine_tuning/prepared/final/val.jsonl\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_result = train_final(\n",
        "    prepared_dir=PREPARED_DIR,\n",
        "    base_model=BASE_MODEL,\n",
        "    n_epochs=FT_EPOCHS,\n",
        "    batch_size=FT_BATCH_SIZE,\n",
        "    learning_rate_multiplier=FT_LEARNING_RATE_MULTIPLIER,\n",
        "    poll_seconds=POLL_SECONDS,\n",
        "    max_wait_minutes=MAX_WAIT_MINUTES,\n",
        "    dry_run=not RUN_LIVE_JOBS,\n",
        "    no_wait=NO_WAIT,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8141b06f",
      "metadata": {},
      "source": [
        "## 5) Evaluate model and emit a run file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec056a18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No final model id found yet. Train final model first or set OPENAI_FT_MODEL_V5.\n"
          ]
        }
      ],
      "source": [
        "final_model_path = ARTIFACTS_ROOT / 'final_model.txt'\n",
        "model_id = ''\n",
        "\n",
        "if final_model_path.exists():\n",
        "    model_id = final_model_path.read_text(encoding='utf-8').strip()\n",
        "elif os.environ.get('OPENAI_FT_MODEL_V5'):\n",
        "    model_id = os.environ['OPENAI_FT_MODEL_V5'].strip()\n",
        "\n",
        "if not model_id:\n",
        "    print('No final model id found yet. Train final model first or set OPENAI_FT_MODEL_V5.')\n",
        "else:\n",
        "    eval_output = ARTIFACTS_ROOT / f'eval-{datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")}.json'\n",
        "    report = eval_model(\n",
        "        model=model_id,\n",
        "        datasets=DATASETS,\n",
        "        output=eval_output,\n",
        "        max_samples=MAX_EVAL_SAMPLES,\n",
        "        write_run_file=WRITE_RUN_FILE,\n",
        "        run_tag='v5',\n",
        "        detector_name='v5',\n",
        "    )\n",
        "    print('Saved eval report:', eval_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa70e2a",
      "metadata": {},
      "source": [
        "## 6) Evaluate On Unseen `final` Validation Split Only\n",
        "\n",
        "Use this for a model trained on `final/train` and scored on `final/val` (unseen examples only).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "190e6ef3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote 90 rows to /Users/max/code/bot-or-not/python/artifacts/fine_tuning/prepared/final/val.eval.jsonl\n",
            "combined: total=90 bots=19 humans=71 | TP=19 TN=71 FP=0 FN=0 | acc=100.00% score=76/76 (100.0%)\n",
            "Invalid outputs: 0\n",
            "Saved eval report: /Users/max/code/bot-or-not/python/artifacts/fine_tuning/final-val-only-eval-2026-02-14T04-58-10Z.json\n",
            "Saved report: /Users/max/code/bot-or-not/python/artifacts/fine_tuning/final-val-only-eval-2026-02-14T04-58-10Z.json\n",
            "Metrics: {\n",
            "  \"total\": 90,\n",
            "  \"bots\": 19,\n",
            "  \"humans\": 71,\n",
            "  \"tp\": 19,\n",
            "  \"tn\": 71,\n",
            "  \"fp\": 0,\n",
            "  \"fn\": 0,\n",
            "  \"accuracy\": 100.0,\n",
            "  \"score\": 76,\n",
            "  \"max_score\": 76,\n",
            "  \"pct_max\": 100.0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Set the model ID to evaluate on final val only.\n",
        "MODEL_ID_FINAL_ONLY = 'ft:gpt-4.1-mini-2025-04-14:personal:test:D91ewpzh'\n",
        "\n",
        "final_dir = PREPARED_DIR / 'final'\n",
        "val_sft_path = final_dir / 'val.jsonl'\n",
        "val_meta_path = final_dir / 'val.meta.jsonl'\n",
        "val_eval_path = final_dir / 'val.eval.jsonl'\n",
        "\n",
        "assert val_sft_path.exists(), f'Missing: {val_sft_path}'\n",
        "assert val_meta_path.exists(), f'Missing: {val_meta_path}'\n",
        "\n",
        "# Build eval manifest from val SFT + metadata.\n",
        "val_sft_rows = [json.loads(line) for line in val_sft_path.read_text(encoding='utf-8').splitlines() if line.strip()]\n",
        "val_meta_rows = [json.loads(line) for line in val_meta_path.read_text(encoding='utf-8').splitlines() if line.strip()]\n",
        "\n",
        "assert len(val_sft_rows) == len(val_meta_rows), 'val.jsonl and val.meta.jsonl row count mismatch'\n",
        "\n",
        "eval_rows = []\n",
        "for sft_row, meta_row in zip(val_sft_rows, val_meta_rows):\n",
        "    messages = sft_row['messages']\n",
        "    # Keep only system+user inputs for evaluation calls.\n",
        "    eval_messages = [m for m in messages if m['role'] != 'assistant']\n",
        "    if len(eval_messages) != 2:\n",
        "        raise ValueError('Expected exactly 2 non-assistant messages (system + user)')\n",
        "\n",
        "    label = meta_row.get('label')\n",
        "    if label not in {'BOT', 'HUMAN'}:\n",
        "        # Fallback: derive label from assistant message if metadata is missing label.\n",
        "        assistant_msgs = [m for m in messages if m['role'] == 'assistant']\n",
        "        if not assistant_msgs:\n",
        "            raise ValueError('Missing label in metadata and no assistant message in SFT row')\n",
        "        label = assistant_msgs[0]['content'].strip().upper()\n",
        "\n",
        "    eval_rows.append({\n",
        "        'user_id': meta_row['user_id'],\n",
        "        'dataset_id': int(meta_row['dataset_id']),\n",
        "        'lang': meta_row['lang'],\n",
        "        'label': label,\n",
        "        'full_post_count': int(meta_row['full_post_count']),\n",
        "        'post_count_used': int(meta_row['post_count_used']),\n",
        "        'messages': eval_messages,\n",
        "    })\n",
        "\n",
        "with val_eval_path.open('w', encoding='utf-8') as f:\n",
        "    for row in eval_rows:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f'Wrote {len(eval_rows)} rows to {val_eval_path}')\n",
        "\n",
        "report_path = ARTIFACTS_ROOT / f'final-val-only-eval-{datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")}.json'\n",
        "report = eval_model(\n",
        "    model=MODEL_ID_FINAL_ONLY,\n",
        "    eval_file=val_eval_path,\n",
        "    output=report_path,\n",
        "    write_run_file=False,\n",
        ")\n",
        "\n",
        "print('Saved report:', report_path)\n",
        "print('Metrics:', json.dumps(report['metrics'], indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2b6c34",
      "metadata": {},
      "source": [
        "## 7) Optional: analyze latest v5 run with JS analyzer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43df808f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No v5 run file found yet.\n"
          ]
        }
      ],
      "source": [
        "import shlex\n",
        "\n",
        "run_files = sorted(RUNS_DIR.glob('v5-*.txt'))\n",
        "if not run_files:\n",
        "    print('No v5 run file found yet.')\n",
        "else:\n",
        "    latest_run = run_files[-1]\n",
        "    print('Latest run file:', latest_run)\n",
        "    run_shell(f'bun run js/analysis.ts {shlex.quote(str(latest_run))}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- `run-cv` uses strict pair holdout (`30+32` train vs `31+33` test, and inverse).\n",
        "- Training/eval prompts include **all posts** per user (no cap).\n",
        "- JS runtime can use the final model via `OPENAI_FT_MODEL_V5`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.13 (bot-or-not)",
      "language": "python",
      "name": "bot-or-not"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
